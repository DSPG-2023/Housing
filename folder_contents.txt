This is an explanation of what is in each folder in the housing repository as of July 2023

.idea:

beacon_sort:
	beacon_image_graber.py:
		takes csv file from beacon, this csv needs addresses and urls to beacon images. Ideally the addresses are clean (123 main st) no extra spaces or words and SKIP is in front of incorrect addresses. 

city address raw data:
	folders for Grundy Center, Independence, New Hampton, Slater, Elkhart, and Des Moines. Each folder has csv files with addresses. This folder was used to clean address data so that we could move them to the complete links folder and edit the addresses to be used as Google API urls.

complete links:
	csv files for Des Moines, Elkhart, Grundy, Independence, Hampton, Ogden, and Slater. These csv files all have a url and address column which are used for the Grab images R file. Grab images uses the links in csv files to download images from the Google API in a new folder. Grab images also renames these images in the format Source_City_Address_ for example and image downloaded from Google for Slater will be in the format G_S_123 Main St_.

demographic analysis:
	Community Profile Datasets:
		csv files that looks like binary 
	Community Summaries:
		grundy, independence, hampton, and iowa community summaries
	profile plots:
		Images of demographic analysis

docs:
	HTML doc for housing landing page

Housing Databases:
	csv files for elkhart, grundy, independence, hampton, ogden, and slater which keep track of attributes of addresses including predictions made by the AI models (inside of models_algorithm). data_sample has a small sample of the cities so that we could evaluate how accurate the models were predicting. 

	sample databases:
		basic database sample csv

models_algorithm:
	folders that begin with model have an .h5 file which is a trained AI model plus the .ipynb file which is the code to train the model which can be opened in Google Colab.
	single_house_image_inputs and clear_images are test images.
	panda_data is images of pandas to test the class_activation_maps.py file. This file makes CAM images which shows what looks like a heatmap over an image to display what areas influence the AI's prediction most.
	PyTorch:
		.py code and .pth model which uses pytorch to try and replicate what is done in the class activation maps but with our own models and house images.
	cam_houses.py is my first attempt to do what is in the PyTorch folder, it does not work but may be helpful to look at while learning about these models.
	single_house_evaluator is the original python script to run all of the models, it has since been updated and split into city_evaluator.py and house_evaluator.py 
	city_evaluator.py will take a folder that stores city folders, inside the city folders are the addresses in that city as folders, inside of address folders are the images for that address which we collected. city_eval will then call house_evaluator.py which is explained next, house_eval returns each address assessed on the quality of gutter, vegetation, siding, and roofing according to our models. city_evaluator then prints all of these results to a csv on the correct address.
	house_evaluator.py takes an address folder as input. First it uses the models house_present, clear_image, and multiple_houses to try and select the best address for a house to be evaluated. If there is no good picture it will return with all the models as FALSE and 0% confidence. If there are multiple good images one will be randomly selected. The image is then assessed by our vegetation, gutter, roof, and siding models. house_evaluator returns all of the evaluations plus percent confidence on each evaluation.
	image_folder_sorter.py will take a folder of images with the naming convention of source_city_address_ and sort each image into the parent folder of images sorted by city then by address. This is neccesary for running city_evaluator.
	house_addributes_test.csv is simply just a testing csv file so that new features can be tested before used on one of the city databases.

test_images:
	there is a .txt file explaining what should be in this folder named test_cases but essentially this will contain multiple images to test various features of the program from functionality to model accuracy.

web scraping:
	This folder needs to be helped quite a bit. There are multiple R files which are able to scrape parts of Zillow but we were having issues that may only be fixed by building a spider to scrape. This is the only folder I would recommend reading through the code (it's not a lot or complex) or to try and redo this section in future DSPG tasks. Also contains images and info that we were able to scrape some of. 

winvest_sort:
	first there are the 3 csv files from the winvest project which have photo name and address columns. After downloading the images rename_winvest.py is able to read these csv files and rename each of the images to our naming convention of source_city_address_






